<pre class='metadata'>
Title: RDF Connect Specification
Shortname: rdfc
Markup Shorthands: markdown yes
Level: 1
Status: LS
Editor: RDF Connect Team, https://example.org
Repository: https://github.com/your-org/rdf-connect
Abstract: Some abstract 
</pre>

# Introduction

RDF Connect is a modular framework for building and executing multilingual data processing pipelines using RDF as the configuration and orchestration layer.

It enables fine-grained, reusable processor components that exchange streaming data, allowing workflows to be described declaratively across programming languages and environments.
RDF Connect is especially well suited for data transformation, integration, and linked data publication.

# Design Goals

* **Language Agnosticism**: Integrate processor components written in different languages (e.g., JavaScript, Python, shell).
* **Streaming by Default**: Pass data between processors, via an orchestrator, to support large-scale and real-time data.
* **Semantic Configuration**: Use RDF to define processors, pipelines, inputs, and outputs.
* **Reusability**: Allow processors to be reused across different pipelines and contexts.
* **Transparency**: Facilitate introspection and documentation of pipeline structure through RDF vocabularies.
* **Lineage**: Achieve native `prov-o` provenance information.

# Concepts

This section introduces the core concepts of the RDF Connect framework.

## Pipeline

A pipeline defines an sequence of data transformation steps.
Each step connects one or more processors with associated runners and configuration parameters.

## Processor

A processor is a reusable software component that accepts input data, transforms it, and emits output data. 
Processors can be written in any language and executed with a runner.

## Runner

A runner is an execution strategy for processors â€” for example, a processor in Javascript is a class that is started using a `NodeRunner`.

## Reader / Writer

Readers and Writers are components that define how data is streamed into and out of a processor.
These provide an idiomatic way to transport streaming data  between processors.

# SHACL as Configuration Schema

RDF Connect uses SHACL [[shacl]] not only as a data validation mechanism but also as a schema language for defining the configuration interface of components such as processors and runners.
These SHACL shapes enable:

* Static validation of component descriptions.
* Programmatic extraction of configuration contracts.
* Type-safe interpretation in environments like JavaScript/TypeScript.

Shapes define required and optional configuration properties, which are transformed into JSON objects at startup, according to the pipeline.


## Mapping SHACL to Configuration Structures

Each `sh:property` statement in a SHACL shape defines a configuration field with the following interpretation:

### Required Fields

**sh:path**
  - Indicates the RDF predicate that must be present on the target resource.
  - Used to extract values from the data graph.

**sh:name**
  - Defines the field name used in the resulting configuration object (e.g., a key in a JSON structure).
  - This decouples the internal RDF predicate from the external configuration representation.

**sh:datatype** or **sh:class**
  - Specifies the expected type of the value.
  - Use `sh:datatype` for primitive types (e.g., `xsd:string`, `xsd:anyURI`).
  - Use `sh:class` for nested resources (i.e., objects that are further interpreted according to their own shapes).

### Optional Fields

**sh:minCount**
  - The minimum number of expected values.
  - If this constraint is not met, a validation error is raised.
  - A value of 1 implies the field is required.

**sh:maxCount**
  - The maximum number of allowed values.
  - If more values are present, a validation error is raised.
  - Special case:
    - If `sh:maxCount = 1`, the resulting field is a single value (`T`).
    - If `sh:maxCount > 1` or is unspecified, the resulting field is an array (`T[]`).

## Example

The following SHACL shape defines the expected configuration for a JavaScript processor:

```turtle
[ ] a sh:NodeShape ;
  sh:targetSubjectsOf rdfc:jsImplementationOf ;
  sh:property [
    sh:path rdfc:entrypoint ;
    sh:name "location" ;
    sh:minCount 1 ;
    sh:maxCount 1 ;
    sh:datatype xsd:string ;
  ], [
    sh:path rdfc:file ;
    sh:name "file" ;
    sh:minCount 1 ;
    sh:maxCount 1 ;
    sh:datatype xsd:string ;
  ], [
    sh:path rdfc:class ;
    sh:name "clazz" ;
    sh:maxCount 1 ;
    sh:datatype xsd:string ;
  ].
```

This results in the following TypeScript-compatible configuration type:

```ts
{
  location: string,
  file: string,
  clazz?: string
}
```

Note that:

* location and file are required.
* clazz is optional due to the absence of `sh:minCount`.

## Nested Shapes and Component Types

Configuration fields can reference structured values instead of primitive literals. 
This is supported via `sh:class`, which indicates that the value must conform to another shape associated with a given RDF class.

For example:

```turtle
sh:property [
  sh:path rdfc:input ;
  sh:name "input" ;
  sh:class rdfc:Reader ;
]
```

This declares a configuration field named input whose value is an instance of `rdfc:Reader`.

### Use of sh:class and `sh:targetClass`

* `sh:class` is used in a property constraint to indicate the expected class of the object (i.e., the RDF resource at that property).
* `sh:targetClass` is used on a `sh:NodeShape` to associate it with a class, so that tools can look up the shape when encountering a value with that class.
* In RDF Connect, `sh:targetClass` shapes define reusable schemas for configuration blocks that can be nested using sh:class.

This allows composable, type-safe configuration structures where nested components follow their own validation and extraction rules.

### Special Component Types: rdfc:Reader and rdfc:Writer

`rdfc:Reader` and `rdfc:Writer` are special component types in RDF Connect. 
Unlike generic nested objects, these represent runtime injection points for data input and output, respectively.

* Fields with `sh:class rdfc:Reader` or `rdfc:Writer` must be instantiated by the runner environment.
* The runner must resolve these values into programming language-native abstractions for sending or receiving data messages.

These abstractions typically expose idiomatic APIs depending on the language (e.g., JavaScript streams, async iterators, function callbacks).

```turtle
sh:property [
  sh:path rdfc:input ;
  sh:name "input" ;
  sh:class rdfc:Reader ;
  sh:minCount 1 ;
]
```

This configuration ensures that the processor receives a data source named input, which will be resolved by the runner and injected into the processor's runtime context.

This approach enables loose coupling between declarative pipeline configuration and concrete data-flow mechanisms.



# Getting Started

This section provides a high-level overview of how to define and run a pipeline in RDF Connect. The rest of the specification provides detail on how each part works.

Here's a simple example:

```turtle
ex:pipeline a rdfc:Pipeline ;
    rdfc:instantiates ex:myRunner ;
    rdfc:processor ex:myProcessor .
```

# RDF Connect by Layer

## Runner

A runner describes how to execute a processor. Most users will reuse existing runners, such as NodeRunner or BashRunner.

You only need to write your own runner if you're integrating a new execution environment.


## Processor

A processor is the reusable data transformation component. Each processor must be associated with a runner.

Even if you're not writing processors yourself, you may need to understand this section to configure one.

## Used In Pipelines

Once defined, a processor can be linked into a pipeline via the `rdfc:processor` property.

```turtle
rdfc:processor ex:myProcessor .
```

## Pipeline

Pipelines connect processors using runners to form a processing graph.

This is the main unit most users interact with when defining workflows.

# Ontology Reference

The RDF Connect ontology provides the terms used in RDF pipeline definitions. See the full [RDF Connect Ontology](https://w3id.org/rdf-connect/ontology.ttl) for details.

# Usage Paths

Depending on your use case, you may only need a subset of this specification:

* **Pipeline Authors**: Start with [[#getting-started]] and focus on [[#pipeline]].
* **Processor Developers**: Read [[#processor]] and [[#runner]].
* **Platform Maintainers**: Read all sections, including implementation notes.

## Putting It All Together: Example Flow and Use Case




