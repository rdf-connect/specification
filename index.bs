<pre class='metadata'>
Title: RDF-Connect Specification
Shortname: rdfc
Markup Shorthands: markdown yes
Level: 1
Status: LS
Editor: RDF-Connect Team, https://example.org
Repository: https://github.com/your-org/rdf-connect
Abstract: Some abstract 
</pre>

<link rel=stylesheet href="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.css">

# Introduction

RDF-Connect is a modular framework for building and executing multilingual data processing pipelines using RDF as the configuration and orchestration layer.

It enables fine-grained, reusable processor components that exchange streaming data, allowing workflows to be described declaratively across programming languages and environments.
RDF-Connect is especially well suited for data transformation, integration, and linked data publication.

# Usage Paths

<div class=note>
This section is complete in terms of content, but may be reorganized or rewritten for clarity during editorial review.
</div>

Depending on your use case, you may only need a subset of this specification:

* **Pipeline Authors**: Start with [[#getting-started]] and focus on [[#pipeline]].
* **Processor Developers**: Read [[#processor]] and [[#runner]].
* **Platform Maintainers**: Read all sections, including implementation notes.

# Design Goals

<div class=note>
This section is complete in terms of content, but may be reorganized or rewritten for clarity during editorial review.
</div>

* **Language Agnosticism**: Integrate processor components written in different languages (e.g., JavaScript, Python, shell).
* **Streaming by Default**: Pass data between processors, via an orchestrator, to support large-scale and real-time data.
* **Semantic Configuration**: Use RDF to define processors, pipelines, inputs, and outputs.
* **Reusability**: Allow processors to be reused across different pipelines and contexts.
* **Transparency**: Facilitate introspection and documentation of pipeline structure through RDF vocabularies.
* **Lineage**: Achieve native `prov-o` provenance information.

# Concepts

This section introduces the core concepts of the RDF-Connect framework.

## Pipeline

A pipeline defines an sequence of data transformation steps.
Each step connects one or more processors with associated runners and configuration parameters.

## Processor

A processor is a reusable software component that accepts input data, transforms it, and emits output data. 
Processors can be written in any language and executed with a runner.

## Runner

A runner is an execution strategy for processors â€” for example, a processor in Javascript is a class that is started using a `NodeRunner`.

## Orchestrator

The orchestrator is the core component responsible for executing a pipeline. 
It reads the configuration, initializes runners, dispatches processor instantiations, and coordinates data flow between them. 
It acts as the runtime conductor that interprets RDF-Connectâ€™s declarative configuration.

## Reader / Writer

Readers and Writers are components that define how data is streamed into and out of a processor.
These provide an idiomatic way to transport streaming data  between processors.


# Getting Started

<div class=issue>
ðŸš§ This section is a work in progress and will be expanded soon.
</div>

This section provides a high-level overview of how to define and run a pipeline in RDF-Connect. The rest of the specification provides detail on how each part works.

Here's a simple example:

```turtle
ex:pipeline a rdfc:Pipeline ;
    rdfc:instantiates ex:myRunner ;
    rdfc:processor ex:myProcessor .
```

Once a pipeline is fully described using RDF, it is handed off to the orchestrator.
The orchestrator parses the configuration, resolves all runner and processor definitions, and initiates execution.


# SHACL as Configuration Schema

RDF-Connect uses SHACL [[shacl]] not only as a data validation mechanism but also as a schema language for defining the configuration interface of components such as processors and runners.
These SHACL shapes enable:

* Static validation of component descriptions.
* Programmatic extraction of configuration contracts.
* Type-safe interpretation in environments like JavaScript/TypeScript.

Shapes define required and optional configuration properties, which are transformed into JSON objects at startup, according to the pipeline.

<div class="example" title="From SHACL to JSON configuration">
    This SHACL shape definition defines a configuration structure for a processor.
    In RDF-Connect, such shapes are used to describe required parameters.
    They result in a well-typed JSON object that developers can rely on during implementation.

    Shacl shape defining some required configuration for a processor

```turtle
[] a sh:NodeShape;
    sh:targetClass <FooBar>;
    sh:property [
        sh:name "repeat";         # JSON field name
        sh:datatype xsd:integer;  # specify datatype
        sh:maxCount 1;
        sh:path :repeat;
    ], [
        sh:name "messages";
        sh:datatype xsd:string;
        sh:path :msg;
    ].
```

Processor configuration
```turtle
<MyProcessor> a <FooBar>;
    :repeat 10;
    :msg "Hello", "World".
```

Results in the following JSON structure.
```typescript
{ 
    "repeat": 10,
    "messages": [ "Hello", "World"]
}
```
</div>


## Mapping SHACL to Configuration Structures

Each `sh:property` statement in a SHACL shape defines a configuration field with the following interpretation:

### Required Fields

**sh:path**
  - Indicates the RDF predicate that must be present on the target resource.
  - Used to extract values from the data graph.

**sh:name**
  - Defines the field name used in the resulting configuration object (e.g., a key in a JSON structure).
  - This decouples the internal RDF predicate from the external configuration representation.

**sh:datatype** or **sh:class**
  - Specifies the expected type of the value.
  - Use `sh:datatype` for primitive types (e.g., `xsd:string`, `xsd:anyURI`).
  - Use `sh:class` for nested resources (i.e., objects that are further interpreted according to their own shapes).

### Optional Fields

**sh:minCount**
  - The minimum number of expected values.
  - If this constraint is not met, a validation error is raised.
  - A value of 1 implies the field is required.

**sh:maxCount**
  - The maximum number of allowed values.
  - If more values are present, a validation error is raised.
  - Special case:
    - If `sh:maxCount = 1`, the resulting field is a single value (`T`).
    - If `sh:maxCount > 1` or is unspecified, the resulting field is an array (`T[]`).


## Nested Shapes and Component Types

Configuration fields can reference structured values instead of primitive literals. 
This is supported via `sh:class`, which indicates that the value must conform to another shape associated with a given RDF class.

For example:

```turtle
sh:property [
  sh:path rdfc:input ;
  sh:name "input" ;
  sh:class rdfc:Reader ;
]
```

This declares a configuration field named input whose value is an instance of `rdfc:Reader`.

### Use of sh:class and `sh:targetClass`

* `sh:class` is used in a property constraint to indicate the expected class of the object (i.e., the RDF resource at that property).
* `sh:targetClass` is used on a `sh:NodeShape` to associate it with a class, so that tools can look up the shape when encountering a value with that class.
* In RDF-Connect, `sh:targetClass` shapes define reusable schemas for configuration blocks that can be nested using sh:class.

This allows composable, type-safe configuration structures where nested components follow their own validation and extraction rules.

### Special Component Types: rdfc:Reader and rdfc:Writer

`rdfc:Reader` and `rdfc:Writer` are special component types in RDF-Connect. 
Unlike generic nested objects, these represent runtime injection points for data input and output, respectively.

* Fields with `sh:class rdfc:Reader` or `rdfc:Writer` must be instantiated by the runner environment.
* The runner must resolve these values into programming language-native abstractions for sending or receiving data messages.

These abstractions typically expose idiomatic APIs depending on the language (e.g., JavaScript streams, async iterators, function callbacks).

```turtle
sh:property [
  sh:path rdfc:input ;
  sh:name "input" ;
  sh:class rdfc:Reader ;
  sh:minCount 1 ;
]
```

This configuration ensures that the processor receives a data source named input, which will be resolved by the runner and injected into the processor's runtime context.

This approach enables loose coupling between declarative pipeline configuration and concrete data-flow mechanisms.


## Example: Putting it all together

The following example SHACL shape could accompany a FooBar processor that appends text to each incoming message and echos it to the next channel.

```turtle
[ ] a sh:NodeShape;
  sh:targetClass :Channels;
  sh:property [
    sh:path rdfc:input ;
    sh:name "input" ;
    sh:class rdfc:Reader ;
    sh:minCount 1 ;
    sh:maxCount 1 ;
  ], [
    sh:path rdfc:output ;
    sh:name "output" ;
    sh:class rdfc:Writer ;
    sh:minCount 1 ;
    sh:maxCount 1 ;
  ].

[ ] a sh:NodeShape ;
  sh:targetClass <FooBar2> ;
  sh:property [
    sh:path :channel ;
    sh:name "channels" ;
    sh:minCount 1 ;
    sh:class :Channels ;
  ], [
    sh:path :append ;
    sh:name "append" ;
    sh:minCount 1 ;
    sh:maxCount 1 ;
    sh:datatype xsd:string ;
  ].
```

```turtle
<foobar> a <FooBar2>;
    :channel [
        #`a :Channels` is not  required, this is implicit from the definition
        rdfc:input <channel1>;
        rdfc:output <channel2>;
    ], [
        rdfc:input <channel2>;
        rdfc:output <channel1>;
    ];
    :append " World!".
```

This results in the following JSON object:
```typescript
{
  channels: [
        { 
            "input": { /* idiomatic input stream for channel <channel1> */ } ,
            "output": {/* idiomatic output stream for channel <channel2> */ } ,
        }, {
            "input": { /* idiomatic input stream for channel <channel2> */ } ,
            "output": {/* idiomatic output stream for channel <channel1> */ } ,
        }
  ],
  append: " World!"
}
```



# RDF-Connect by Layer


## Orchestrator

The orchestrator is the central runtime entity in RDF-Connect. 
It reads the pipeline configuration, sets up the runners, initiates processors, and routes messages between them.
It ensures the dataflow graph described by the pipeline is brought to life across isolated runtimes.
The orchestrator acts as a coordinator, not an executor. Each runner is responsible for running the actual processor code, but the orchestrator ensures the pipeline as a whole behaves as intended.

Responsibilities:

* Parse the pipeline RDF.
* Load SHACL shapes for processors and runners.
* Validate and coerce configuration to structured JSON.
* Instantiate runners.
* Start processors.
* Mediate messages (data and control).
* Handle retries, streaming, and backpressure.

<div class=note>
The remained of this section is intended for developers building custom runners or integrating RDF-Connect into infrastructure.
</div>

### Protobuf Messaging Protocol

Communication between the orchestrator and the runners happens using a strongly typed protocol defined in Protocol Buffers (protobuf).
This enables language-independent and efficient communication across processes and machines.

### Startup Flow

The following diagram describes the startup sequence handled by the orchestrator. This includes validating pipeline structure, instantiating runners, and initializing processor instances.

<pre class=include>
path: ./startup.mdd
</pre>

### Message Handling Flow

Once processors are running, the orchestrator handles incoming messages and forwards them to the appropriate reader instances, based on their declared channels.

<pre class=include>
path: ./message.mdd
</pre>


### Streaming Messages

For large messages or real-time input, RDF-Connect supports a streaming model.
Instead of sending entire payloads as a single message, the message can be broken into chunks and sends them over time. 
This is handled by the StreamChunk message type.

<pre class=include>
path: ./streamMessage.mdd
</pre>


## Runner

A runner describes how to execute a processor. Most users will reuse existing runners, such as NodeRunner or BashRunner.

You only need to write your own runner if you're integrating a new execution environment.


## Processor

A processor is the reusable data transformation component. Each processor must be associated with a runner.

Even if you're not writing processors yourself, you may need to understand this section to configure one.

## Used In Pipelines

Once defined, a processor can be linked into a pipeline via the `rdfc:processor` property.

```turtle
rdfc:processor ex:myProcessor .
```

## Pipeline

Pipelines connect processors using runners to form a processing graph.

This is the main unit most users interact with when defining workflows.


# Ontology Reference

The RDF-Connect ontology provides the terms used in RDF pipeline definitions. See the full [RDF-Connect Ontology](https://w3id.org/rdf-connect/ontology.ttl) for details.


# Putting It All Together: Example Flow and Use Case

<div class=issue>
ðŸš§ This section is a work in progress and will be expanded soon.
</div>




<script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>
